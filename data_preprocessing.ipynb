{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " データ前処理\n",
    "\n",
    "1. 欠損値処理\n",
    "2. 外れ値検出と処理\n",
    "3. データの標準化・正規化\n",
    "4. データの型変換\n",
    "5. テキストデータの前処理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のデータ:\n",
      "      A    B\n",
      "0  1.0  NaN\n",
      "1  2.0  2.0\n",
      "2  NaN  3.0\n",
      "3  4.0  4.0\n",
      "4  5.0  NaN\n",
      "平均値補完:\n",
      "      A    B\n",
      "0  1.0  3.0\n",
      "1  2.0  2.0\n",
      "2  3.0  3.0\n",
      "3  4.0  4.0\n",
      "4  5.0  3.0\n",
      "前のデータで補完:\n",
      "      A    B\n",
      "0  1.0  NaN\n",
      "1  2.0  2.0\n",
      "2  2.0  3.0\n",
      "3  4.0  4.0\n",
      "4  5.0  4.0\n",
      "欠損値を含む行の削除:\n",
      "      A    B\n",
      "1  2.0  2.0\n",
      "3  4.0  4.0\n"
     ]
    }
   ],
   "source": [
    "# 1. 欠損値処理 (平均値補完、前後データ補完、削除)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# サンプルデータ作成\n",
    "data = {'A': [1, 2, np.nan, 4, 5],\n",
    "        'B': [np.nan, 2, 3, 4, np.nan]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 平均値補完\n",
    "df_mean = df.fillna(df.mean())\n",
    "# 前のデータで補完\n",
    "df_ffill = df.fillna(method='ffill')\n",
    "# 欠損値を含む行を削除\n",
    "df_drop = df.dropna()\n",
    "\n",
    "print('元のデータ:\\n', df)\n",
    "print('平均値補完:\\n', df_mean)\n",
    "print('前のデータで補完:\\n', df_ffill)\n",
    "print('欠損値を含む行の削除:\\n', df_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR法による外れ値除去:\n",
      "      A    B\n",
      "0  1.0  NaN\n",
      "1  2.0  2.0\n",
      "2  NaN  3.0\n",
      "3  4.0  4.0\n",
      "4  5.0  NaN\n",
      "Zスコアによる外れ値除去:\n",
      "      A    B\n",
      "0  1.0  NaN\n",
      "1  2.0  2.0\n",
      "2  NaN  3.0\n",
      "3  4.0  4.0\n",
      "4  5.0  NaN\n"
     ]
    }
   ],
   "source": [
    "# 2. 外れ値検出と処理 (IQR法, Zスコア)\n",
    "from scipy import stats\n",
    "\n",
    "# IQR法による外れ値検出\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df_no_outliers = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Zスコアによる外れ値検出\n",
    "z_scores = np.abs(stats.zscore(df.fillna(0)))\n",
    "df_zscore = df[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "print('IQR法による外れ値除去:\\n', df_no_outliers)\n",
    "print('Zスコアによる外れ値除去:\\n', df_zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Maxスケーリング:\n",
      "      A     B\n",
      "0  0.2  0.00\n",
      "1  0.4  0.50\n",
      "2  0.0  0.75\n",
      "3  0.8  1.00\n",
      "4  1.0  0.00\n",
      "Zスコア正規化:\n",
      "           A      B\n",
      "0 -0.754829 -1.125\n",
      "1 -0.215666  0.125\n",
      "2 -1.293993  0.750\n",
      "3  0.862662  1.375\n",
      "4  1.401826 -1.125\n"
     ]
    }
   ],
   "source": [
    "# 3. データの標準化・正規化 (Min-Maxスケーリング, Zスコア正規化)\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Min-Maxスケーリング\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_minmax = pd.DataFrame(scaler_minmax.fit_transform(df.fillna(0)), columns=df.columns)\n",
    "\n",
    "# Zスコア正規化\n",
    "scaler_zscore = StandardScaler()\n",
    "df_zscore_scaled = pd.DataFrame(scaler_zscore.fit_transform(df.fillna(0)), columns=df.columns)\n",
    "\n",
    "print('Min-Maxスケーリング:\\n', df_minmax)\n",
    "print('Zスコア正規化:\\n', df_zscore_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のカテゴリデータ:\n",
      "    Color\n",
      "0    Red\n",
      "1   Blue\n",
      "2  Green\n",
      "3   Blue\n",
      "4    Red\n",
      "One-Hotエンコーディング後:\n",
      "    Color_Blue  Color_Green  Color_Red\n",
      "0       False        False       True\n",
      "1        True        False      False\n",
      "2       False         True      False\n",
      "3        True        False      False\n",
      "4       False        False       True\n"
     ]
    }
   ],
   "source": [
    "# 4. データの型変換 (カテゴリデータのOne-Hotエンコーディング)\n",
    "df_cat = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']})\n",
    "\n",
    "# One-Hotエンコーディング\n",
    "df_encoded = pd.get_dummies(df_cat)\n",
    "\n",
    "print('元のカテゴリデータ:\\n', df_cat)\n",
    "print('One-Hotエンコーディング後:\\n', df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. テキストデータの前処理 (トークナイゼーション, ストップワード除去, ステミング)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# サンプルテキスト\n",
    "text = \"This is a simple example demonstrating text preprocessing.\"\n",
    "\n",
    "# トークナイゼーション\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# ストップワード除去\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# ステミング\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "print('トークナイゼーション:', tokens)\n",
    "print('ストップワード除去後:', filtered_tokens)\n",
    "print('ステミング後:', stemmed_tokens)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
